# :herb: Hadoop

- 대용량 데이터를 분산 처리할 수 있는 자바기반의 오픈소스 프레임워크
- Apache 프로젝트의 MapReduce 프레임워크 오픈소스

- 하나의 대형 컴퓨터를 사용하여 데이터를 처리 및 저장하는 대신, 하둡을 사용하면 상용 하드웨어를 함께 클러스터링하여 대량의 데이터 세트를 병렬로 분석 가능

- 한 번에 여러 디스크로부터 데이터를 읽기 가능

- HDFS(Hadoop Distributed File System)라는 데이터 저장소와 맵리듀스(MapReduce)라는 분석 시스템을 통해 분산 프로그래밍을 수행하는 프레임 워크





## 특징

- Distributed
  - 수십만대의 컴퓨터에 자료 분산 저장 및 처리
- Scalable
  - 용량이 증대되는 대로 컴퓨터 추가
  - scalable하다 : 사용자 수가 급증하거나 데이터가 급증해도 프로그램이 멈추거나 성능이 크게 떨어지는 일이 없다
- Fault-tolerant
  - 시스템을 구성하는 부품의 일부에서 결함(fault), 고장(failure)이 발생하여도 정상적 혹은 부분적으로 기능을 수행할 수 있는 것
- Open source





## 분산 저장

- 하둡 분산 파일 시스템(HDFS-Hadoop Distributed File System)
  - 빅 데이터 파일을 **적당한 블록 사이즈(64MB)**로 나눠서 각 노드 클러스터(각각의 개별 컴퓨터)에 저장
  - 각 파일은 여러 개의 순차적인 블록으로 저장
  - 하나의 파일의 각각의 블록은 fault tolerance를 위해서 여러 개로 복사(최소 3카피)되어 여러 머신의 여기저기 저장됨
  - 빅 데이터를 수천 대의 값싼 컴퓨터에 병렬 처리하기 위해 분산함





## 분산 처리

- MapReduce 프레임워크를 이용해서 계산

- 분산 처리를 위해서 프레임워크에 맞춰서 코딩을 하고 하둡 시스템에서 그것을 실행하면 자동으로 분산 처리를 해 준다.
- Map 함수에서 데이터를 처리하고 Reduce 함수에서 원하는 결과값을 계산





## 구성 요소

- MapReduce 
  - 소프트웨어의 수행을 분산
- HDFS(Hadoop Distributed File System)
  - 데이터를 분산
- 한 개의 Namenode(master)와 여러 개의 Datanode(slaves)
  - Namenode
    - master sever
    - 파일 시스템을 관리하고 클라이언트가 파일에 접근할 수 있게 함
  - Datanode
    - slave sever
    - 컴퓨터에 들어있는 데이터를 접근할 수 있게함
- 자바 프로그래밍 언어로 MapReduce 알고리즘 구현





## 사용 이유

기존 RDBMS는 비싸지만, 하둡은 싸다

RDBMS는 빅 데이터의 대용량 데이터를 감당하기엔 비용이 크다 

하둡은 **분산컴퓨팅 방식**을 이용해서 저렴한 구축 비용과 비용 대비 빠른 데이터 처리, 그리고 장애를 대비한 특성을 갖추고 있다.

하둡은 트랜젝션이나 무결성을 보장해야하는 것들이 아닌 **배치성으로 저장되는 데이터에 적합**한 시스템

ex) 회원 가입, 결제 진행 -> 트랜젝션이나 무결성 보장 -> RDBMS

​		회원이 관심있게 보는 물품, 이동경로, 머무르는 시간 등 -> 배치성 -> 하둡

배치성으로 저장되는 데이터들을 매번 비싼 RDBMS에 저장하면 낭비 요소

따라서, 하둡은 **RDBMS**와 경쟁하는 것이 아닌 **협력하는 것**